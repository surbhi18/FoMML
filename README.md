---
layout: home
title: Foundations of Modern Machine Learning
nav_exclude: true
permalink: /:path/
seo:
  type: CIS 7000
  name: Foundations of Modern Machine Learning - Theory and Empirics
---

- **Lecture time**: Tuesdays and Thursdays 1:45-3:15 PM
- **Lecture location**: Zoom (for September), DRLB 3C8
- **Instructors**: [Surbhi Goel](https://www.surbhigoel.com) (surbhig)
- **Instructor office hours**: Tuesday at 10:30-11:30AM (Zoom / Levine 505)
- **Questions**: We will be using Ed Discussion for all course communications. You can post privately on Ed to contact me, when needed.

**Waitlist information**: This course is capped at 25, and will require active participation from the students including role-playing style discussions, and paper presentations. If you are on the waitlist and you think the course is a good fit for you, please email me.

## Course Overview

This advanced graduate-level course will focus on the latest theoretical and empirical developments in modern machine learning, with a primary focus on deep learning. We will explore cutting-edge machine learning methods such as transformers, diffusion models, self-supervised learning, and more, with an emphasis on the conceptual basis behind their design, success, and limitations. In addition to evaluating in-distribution performance, we will also assess out-of-distribution performance, robustness, emergent behavior, and optimization challenges and mysteries of these models. We will cover classical theoretical results in statistical machine learning and optimization, and explore their connections (or lack thereof) to recent empirical and theoretical research findings.

The [course schedule](calendar.md) contains the tentative schedule including topics we will cover in the lectures and relevant reading material. Please refer [here](about.md) for the more information on the course along with the course policies.

## Format
This course will feature a combination of lectures, and student presentations, with a significant portion of class time dedicated to in-depth discussions of the presented material. There will also be a few guest lectures from leading researchers from industry and academia. The paper discussions will involve role-playing inspired by [Alec Jacobson and Colin Raffel](https://colinraffel.com/blog/role-playing-seminar.html). We will be adopting a subset of the following roles:

- Reviewer: The paper has been submitted to NeurIPS, and you have been assigned to review it. Follow the [review form](https://neurips.cc/Conferences/2023/ReviewerGuidelines) from NeurIPS as a guideline and submit a review for this paper. 
- Archaeologist: Youâ€™re an archeologist who must determine where this paper sits in the context of previous work. You must find and report on atleast one _older_ paper cited within the current paper that substantially influenced the current paper and atleast one _newer_ paper that cites this current paper. Look out for follow-up work that offers criticism of the current paper.
- Student Researcher: You are looking for a new research problem and the paper piqued your interest. You have to come up with follow-up project ideas based on the paper. These can be direct improvements of the paper, or projects that use the papers results as building blocks.
- Reproducibility Checker: You want to check whether the claims of the paper are valid. You decide to create a small experiment (toy dataset, or toy model) and verify if the results hold. Make sure to check how robust they are to various choice of hyperparameters.
- Quanta Correspondent: You want to write an article on the paper for Qanta. You must explain to a broad audience the main problem the paper is addressing, why it is impotant, and what the main technical contribution is.

## Prerequisites
There are no official prerequisites but a good grasp of probability, linear algebra, machine learning is expected. The following books are useful for background reading.

- Machine Learning (ML) by Tom Mitchell. Available as PDF [here](http://www.cs.cmu.edu/~tom/mlbook.html). A classic introduction to machine learning that assumes no knowledge of statistics or artificial intelligence. 
- Elements of Statistical Learning (ESL) by Trevor Hastie, Robert Tibshirani and Jerome Friedman. Available as PDF [here](https://hastie.su.domains/Papers/ESLII.pdf).
- Probabilistic Machine Learning: An Introduction (PML) by Kevin Murphy. Available as PDF [here](https://probml.github.io/pml-book/book1.html).
- Understanding Machine Learning: From Theory to Algorithms (UML) by Shai Shalev-Shwartz and Shai Ben-David. Available as PDF [here](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf). Refer to this book for a more detailed theoretical exposition of the material covered in class.
- Mathematics for Machine Learning (MML) by Marc Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Available as PDF [here](https://mml-book.github.io/book/mml-book.pdf).
- Linear Algebra Review and Reference by Zico Kolter. Available as PDF [here](http://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf).

## Resources 

We will be reading several seminal works in machine learning as well as very recent works that have not yet been peer-reviewed. Therefore, learning how to effectively read and review papers will be a focus of the class. [Learning Theory Alliance](www.let-all.com) has some very helpful resources on tips for effectively reading papers ([read1](https://let-all.com/assets/slides/How-to-ALT22-Aaditya.pdf) and [read2](https://let-all.com/assets/slides/How-to-ALT22-Sam.pdf)) and reviewing ([review1](https://let-all.com/assets/slides/How-to-ALT22-Csaba.pdf)) that I encourage you to check out.
